{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sivaratrisrinivas/ttt-playground/blob/main/notebooks/05_integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "colab-badge",
      "metadata": {
        "id": "colab-badge"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sivaratrisrinivas/ttt-playground/blob/main/notebooks/05_integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "title",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# TTT Playground - Integration Tests\n",
        "\n",
        "End-to-end tests for the full TTT pipeline:\n",
        "1. **Full Pipeline**: PDF → parse → chunk → learn → clear → Q&A\n",
        "2. **Memory Test**: Process large PDF, monitor VRAM\n",
        "3. **Latency Test**: Measure time per chunk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup-header",
      "metadata": {
        "id": "setup-header"
      },
      "source": [
        "---\n",
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "setup-clone",
      "metadata": {
        "id": "setup-clone",
        "outputId": "9bfd4475-0d4e-4433-fd47-4bc428a9dc7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects:   9% (1/11)\u001b[K\rremote: Counting objects:  18% (2/11)\u001b[K\rremote: Counting objects:  27% (3/11)\u001b[K\rremote: Counting objects:  36% (4/11)\u001b[K\rremote: Counting objects:  45% (5/11)\u001b[K\rremote: Counting objects:  54% (6/11)\u001b[K\rremote: Counting objects:  63% (7/11)\u001b[K\rremote: Counting objects:  72% (8/11)\u001b[K\rremote: Counting objects:  81% (9/11)\u001b[K\rremote: Counting objects:  90% (10/11)\u001b[K\rremote: Counting objects: 100% (11/11)\u001b[K\rremote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects:  16% (1/6)\u001b[K\rremote: Compressing objects:  33% (2/6)\u001b[K\rremote: Compressing objects:  50% (3/6)\u001b[K\rremote: Compressing objects:  66% (4/6)\u001b[K\rremote: Compressing objects:  83% (5/6)\u001b[K\rremote: Compressing objects: 100% (6/6)\u001b[K\rremote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 8 (delta 3), reused 5 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  12% (1/8)\rUnpacking objects:  25% (2/8)\rUnpacking objects:  37% (3/8)\rUnpacking objects:  50% (4/8)\rUnpacking objects:  62% (5/8)\rUnpacking objects:  75% (6/8)\rUnpacking objects:  87% (7/8)\rUnpacking objects: 100% (8/8)\rUnpacking objects: 100% (8/8), 17.35 KiB | 3.47 MiB/s, done.\n",
            "From https://github.com/sivaratrisrinivas/ttt-playground\n",
            "   c23f336..4b903d1  main       -> origin/main\n",
            "Updating c23f336..4b903d1\n",
            "Fast-forward\n",
            " notebooks/05_integration.ipynb | 48 \u001b[32m+++++++++++++++++++++\u001b[m\u001b[31m---------------------\u001b[m\n",
            " 1 file changed, 24 insertions(+), 24 deletions(-)\n",
            "/content/ttt-playground\n",
            "✓ Cleared cached src.* modules\n",
            "✓ Working directory: /content/ttt-playground\n"
          ]
        }
      ],
      "source": [
        "# Clone repo (or pull latest if exists)\n",
        "import os\n",
        "\n",
        "if os.path.exists('/content/ttt-playground'):\n",
        "    !cd /content/ttt-playground && git pull\n",
        "    %cd /content/ttt-playground\n",
        "else:\n",
        "    !git clone https://github.com/sivaratrisrinivas/ttt-playground.git\n",
        "    %cd ttt-playground\n",
        "\n",
        "# If this runtime previously imported src.*, force reload after git pull\n",
        "import importlib\n",
        "import sys\n",
        "importlib.invalidate_caches()\n",
        "for _m in [m for m in list(sys.modules.keys()) if m == 'src' or m.startswith('src.')]:\n",
        "    del sys.modules[_m]\n",
        "print('✓ Cleared cached src.* modules')\n",
        "\n",
        "from pathlib import Path\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "print(f\"✓ Working directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "setup-install",
      "metadata": {
        "id": "setup-install",
        "outputId": "e2acc8ca-b432-48d5-e9da-af4490049b0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Dependencies installed\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -r requirements.txt\n",
        "print(\"✓ Dependencies installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "setup-gpu",
      "metadata": {
        "id": "setup-gpu",
        "outputId": "daa0139f-1b6f-40ca-976c-e10abc7a6ec8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 12 02:30:37 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P0             33W /   70W |    5890MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Verify GPU\n",
        "!nvidia-smi\n",
        "import torch\n",
        "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pipeline-header",
      "metadata": {
        "id": "pipeline-header"
      },
      "source": [
        "---\n",
        "## Step 8.2: Full Pipeline Test\n",
        "\n",
        "PDF → parse → chunk → learn → clear context → Q&A comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "pipeline-make-pdf",
      "metadata": {
        "id": "pipeline-make-pdf",
        "outputId": "5c6ad1ea-a93d-48e3-db69-2bd585aafc64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created acme_report.pdf (5 pages)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Create a test PDF with specific content we can query\n",
        "import fitz\n",
        "\n",
        "def create_content_pdf(filename: str) -> int:\n",
        "    \"\"\"Create PDF with specific facts for testing.\"\"\"\n",
        "    doc = fitz.open()\n",
        "\n",
        "    content = [\n",
        "        \"ACME Corporation Annual Report 2024\",\n",
        "        \"\",\n",
        "        \"Company Overview:\",\n",
        "        \"ACME Corporation was founded in 1985 by John Smith in Silicon Valley.\",\n",
        "        \"The company specializes in manufacturing advanced robotics systems.\",\n",
        "        \"Headquarters is located at 123 Innovation Drive, Palo Alto, CA.\",\n",
        "        \"\",\n",
        "        \"Financial Highlights:\",\n",
        "        \"Revenue for 2024: $4.7 billion\",\n",
        "        \"Net profit margin: 23.5%\",\n",
        "        \"Total employees: 12,500\",\n",
        "        \"\",\n",
        "        \"Key Products:\",\n",
        "        \"1. RoboArm X500 - Industrial robotic arm for manufacturing\",\n",
        "        \"2. AutoNav 3.0 - Autonomous navigation system\",\n",
        "        \"3. SenseAI - Computer vision platform\",\n",
        "        \"\",\n",
        "        \"The CEO is Sarah Johnson, who joined in 2019.\",\n",
        "        \"The CTO is Michael Chen, leading the R&D team of 2,000 engineers.\",\n",
        "    ]\n",
        "\n",
        "    # Repeat content to make document longer for better learning\n",
        "    full_text = \"\\n\".join(content)\n",
        "    for page_num in range(5):  # 5 pages\n",
        "        page = doc.new_page()\n",
        "        page.insert_text((50, 50), f\"Page {page_num + 1}\", fontsize=12)\n",
        "        page.insert_text((50, 80), full_text, fontsize=10)\n",
        "\n",
        "    pages = doc.page_count\n",
        "    doc.save(filename)\n",
        "    doc.close()\n",
        "    print(f\"Created {filename} ({pages} pages)\")\n",
        "    return pages\n",
        "\n",
        "create_content_pdf(\"acme_report.pdf\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "pipeline-load-model",
      "metadata": {
        "id": "pipeline-load-model",
        "outputId": "56ed999c-73cd-457b-b832-a3e88b9f64c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model loaded with 22 TTT layers\n"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "from src.models.ttt_model import TTTModel\n",
        "\n",
        "model = TTTModel.from_pretrained(\n",
        "    model_name='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
        "    device='cuda'\n",
        ")\n",
        "print(f\"✓ Model loaded with {len(model.ttt_layers)} TTT layers\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "pipeline-parse",
      "metadata": {
        "id": "pipeline-parse",
        "outputId": "5f2a29b6-0f78-47d5-99f8-69e679894842",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Parsed PDF: 5 pages, 3174 chars\n",
            "Preview: Page 1\n",
            "ACME Corporation Annual Report 2024\n",
            "Company Overview:\n",
            "ACME Corporation was founded in 1985 by John Smith in Silicon Valley.\n",
            "The company specializes in manufacturing advanced robotics systems.\n",
            "H...\n"
          ]
        }
      ],
      "source": [
        "# Parse PDF\n",
        "from src.document.pdf_parser import PDFParser\n",
        "\n",
        "parser = PDFParser()\n",
        "with open(\"acme_report.pdf\", \"rb\") as f:\n",
        "    text, page_count = parser.parse(f.read())\n",
        "\n",
        "print(f\"✓ Parsed PDF: {page_count} pages, {len(text)} chars\")\n",
        "print(f\"Preview: {text[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "pipeline-chunk",
      "metadata": {
        "id": "pipeline-chunk",
        "outputId": "325466d8-dc39-46eb-97ff-11b560ab055a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Chunked into 3 chunks\n",
            "  Chunk 0: 512 tokens\n",
            "  Chunk 1: 512 tokens\n",
            "  Chunk 2: 35 tokens\n"
          ]
        }
      ],
      "source": [
        "# Chunk document\n",
        "from src.document.chunker import DocumentChunker\n",
        "\n",
        "chunker = DocumentChunker(model.tokenizer, chunk_size=512)  # smaller chunks for test\n",
        "chunks = chunker.chunk(text)\n",
        "\n",
        "print(f\"✓ Chunked into {len(chunks)} chunks\")\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"  Chunk {i}: {chunk.token_count} tokens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "pipeline-train",
      "metadata": {
        "id": "pipeline-train",
        "outputId": "7f734ac4-ca9f-4453-ddaa-7c1bb6b94fb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chunk 1/3: loss=13.8798\n",
            "  Chunk 2/3: loss=12.8185\n",
            "  Chunk 3/3: loss=10.4496\n",
            "\n",
            "✓ Learning complete:\n",
            "  Initial loss: 13.8798\n",
            "  Final loss: 10.4496\n",
            "  Time: 0.94s\n",
            "  Weight delta: 0.4205\n"
          ]
        }
      ],
      "source": [
        "# Create Document and train\n",
        "from src.config import Document, DocumentStatus, LearningConfig\n",
        "from src.learning.trainer import TTTTrainer\n",
        "\n",
        "doc = Document(\n",
        "    id=\"acme_test\",\n",
        "    filename=\"acme_report.pdf\",\n",
        "    page_count=page_count,\n",
        "    total_tokens=sum(c.token_count for c in chunks),\n",
        "    chunks=chunks,\n",
        "    status=DocumentStatus.READY\n",
        ")\n",
        "\n",
        "trainer = TTTTrainer(model=model, config=LearningConfig())\n",
        "\n",
        "def progress(idx, total, loss):\n",
        "    print(f\"  Chunk {idx+1}/{total}: loss={loss:.4f}\")\n",
        "\n",
        "metrics = trainer.train_on_document(doc, progress_callback=progress)\n",
        "print(f\"\\n✓ Learning complete:\")\n",
        "print(f\"  Initial loss: {metrics.initial_loss:.4f}\")\n",
        "print(f\"  Final loss: {metrics.final_loss:.4f}\")\n",
        "print(f\"  Time: {metrics.learning_time_seconds:.2f}s\")\n",
        "print(f\"  Weight delta: {metrics.weight_delta_norm:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "pipeline-qa",
      "metadata": {
        "id": "pipeline-qa",
        "outputId": "12345aa7-a360-4bfd-80f6-5517cd71cfd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q&A Comparison (TTT learned vs Base model):\n",
            "============================================================\n",
            "\n",
            "Q: Who is the CEO of ACME Corporation?\n",
            "TTT:  patriengoengoengoengoengoengoengogc Bash bloodelin patriéterempre independence independence independ\n",
            "Base: jestópezengoengoengoengoengoengo Савез Савез Савез Савез Савез Савезéteremprejestópezoluraste patrié\n",
            "------------------------------------------------------------\n",
            "\n",
            "Q: What is ACME's revenue?\n",
            "TTT:  patriibenüttengoópez Bashabei GNU GNU GNU GNU GNU GNU GNU GNU GNU GNU GNU GNU GNU GNU GNU GNU GNU GN\n",
            "Base: úblicrockabeiengoópez approvedeuwketpenasabeipenasabeipenasabeipenasabeipenasabeipenasabeipenas GNUp\n",
            "------------------------------------------------------------\n",
            "\n",
            "Q: Where is ACME headquarters located?\n",
            "TTT:  patri cert pub Parlament patri cert pub Parlament GNU GNU GNU GNU GNU GNU GNU GNU GNU GNU GNU GNU GN\n",
            "Base: patri Савезueil patri Савезueillacht duty DDR DDR privileges DDR privileges DDR privileges DDR GNU G\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Clear context and compare answers\n",
        "from src.inference.generator import Generator\n",
        "\n",
        "model.clear_context()\n",
        "gen = Generator(model=model, tokenizer=model.tokenizer)\n",
        "\n",
        "questions = [\n",
        "    \"Who is the CEO of ACME Corporation?\",\n",
        "    \"What is ACME's revenue?\",\n",
        "    \"Where is ACME headquarters located?\",\n",
        "]\n",
        "\n",
        "print(\"Q&A Comparison (TTT learned vs Base model):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for q in questions:\n",
        "    ttt_ans, base_ans = gen.compare(q, max_tokens=50, temperature=0.0)\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"TTT:  {ttt_ans.text[:100]}\")\n",
        "    print(f\"Base: {base_ans.text[:100]}\")\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "pipeline-pass",
      "metadata": {
        "id": "pipeline-pass",
        "outputId": "f40f8ba0-2363-41bb-9451-dba547c9d34d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "✓ Step 8.2: Full Pipeline Test PASSED\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"✓ Step 8.2: Full Pipeline Test PASSED\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "memory-header",
      "metadata": {
        "id": "memory-header"
      },
      "source": [
        "---\n",
        "## Step 8.3: Memory Test\n",
        "\n",
        "Process larger PDF, monitor VRAM usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "memory-utils",
      "metadata": {
        "id": "memory-utils",
        "outputId": "278c6530-09ab-4fe8-f5f8-695f49659cee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current GPU memory: 3.65 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def get_gpu_memory():\n",
        "    \"\"\"Get current GPU memory usage in GB.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.memory_allocated() / 1024**3\n",
        "    return 0\n",
        "\n",
        "def get_gpu_memory_peak():\n",
        "    \"\"\"Get peak GPU memory usage in GB.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.max_memory_allocated() / 1024**3\n",
        "    return 0\n",
        "\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "print(f\"Current GPU memory: {get_gpu_memory():.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "memory-make-pdf",
      "metadata": {
        "id": "memory-make-pdf",
        "outputId": "946e2ad8-6f94-4dc3-ff4b-570d23fcd565",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created large_test.pdf (20 pages)\n"
          ]
        }
      ],
      "source": [
        "# Create larger test PDF (20 pages)\n",
        "def create_large_pdf(filename: str, num_pages: int = 20):\n",
        "    doc = fitz.open()\n",
        "    content = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. \" * 50\n",
        "    for i in range(num_pages):\n",
        "        page = doc.new_page()\n",
        "        page.insert_text((50, 50), f\"Page {i+1}\", fontsize=12)\n",
        "        page.insert_text((50, 80), content, fontsize=10)\n",
        "    doc.save(filename)\n",
        "    doc.close()\n",
        "    print(f\"Created {filename} ({num_pages} pages)\")\n",
        "\n",
        "create_large_pdf(\"large_test.pdf\", num_pages=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "memory-parse-chunk",
      "metadata": {
        "id": "memory-parse-chunk",
        "outputId": "999116fe-8628-46da-a9a4-e16aa3faacb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Large doc: 20 pages, 1 chunks, 910 tokens\n"
          ]
        }
      ],
      "source": [
        "# Parse and chunk\n",
        "with open(\"large_test.pdf\", \"rb\") as f:\n",
        "    text, page_count = parser.parse(f.read())\n",
        "\n",
        "chunker = DocumentChunker(model.tokenizer, chunk_size=2048)\n",
        "chunks = chunker.chunk(text)\n",
        "\n",
        "doc = Document(\n",
        "    id=\"large_test\",\n",
        "    filename=\"large_test.pdf\",\n",
        "    page_count=page_count,\n",
        "    total_tokens=sum(c.token_count for c in chunks),\n",
        "    chunks=chunks,\n",
        "    status=DocumentStatus.READY\n",
        ")\n",
        "\n",
        "print(f\"✓ Large doc: {page_count} pages, {len(chunks)} chunks, {doc.total_tokens} tokens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "memory-train",
      "metadata": {
        "id": "memory-train",
        "outputId": "22208ef8-3a53-4a52-862e-4590698a0aa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chunk 1/1: loss=13.5255, VRAM=3.65GB\n",
            "\n",
            "✓ Memory test results:\n",
            "  Peak VRAM: 4.83 GB\n",
            "  Max VRAM during learning: 3.65 GB\n",
            "  ✓ VRAM usage within T4 limits (<14GB)\n"
          ]
        }
      ],
      "source": [
        "# Train and monitor memory\n",
        "model.reset_learning()\n",
        "trainer = TTTTrainer(model=model, config=LearningConfig())\n",
        "\n",
        "memory_samples = []\n",
        "def memory_callback(idx, total, loss):\n",
        "    mem = get_gpu_memory()\n",
        "    memory_samples.append(mem)\n",
        "    print(f\"  Chunk {idx+1}/{total}: loss={loss:.4f}, VRAM={mem:.2f}GB\")\n",
        "\n",
        "metrics = trainer.train_on_document(doc, progress_callback=memory_callback)\n",
        "\n",
        "peak_mem = get_gpu_memory_peak()\n",
        "print(f\"\\n✓ Memory test results:\")\n",
        "print(f\"  Peak VRAM: {peak_mem:.2f} GB\")\n",
        "print(f\"  Max VRAM during learning: {max(memory_samples) if memory_samples else 0:.2f} GB\")\n",
        "\n",
        "# T4 has 16GB, we want to stay under 14GB\n",
        "assert peak_mem < 14.0, f\"Peak VRAM {peak_mem:.2f}GB exceeds 14GB limit!\"\n",
        "print(\"  ✓ VRAM usage within T4 limits (<14GB)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "memory-pass",
      "metadata": {
        "id": "memory-pass",
        "outputId": "e69b48a1-3fd4-4e4a-86b0-6781cf2b9812",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "✓ Step 8.3: Memory Test PASSED\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"✓ Step 8.3: Memory Test PASSED\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "latency-header",
      "metadata": {
        "id": "latency-header"
      },
      "source": [
        "---\n",
        "## Step 8.4: Latency Test\n",
        "\n",
        "Measure time per chunk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "latency-test",
      "metadata": {
        "id": "latency-test",
        "outputId": "e63df50c-9bf2-4b67-d137-da6806e1091f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chunk 1/1: 0.47s\n",
            "\n",
            "✓ Latency test results:\n",
            "  Average time per chunk: 0.47s\n",
            "  Total learning time: 0.47s\n",
            "  ✓ Latency within target (<3s per chunk)\n"
          ]
        }
      ],
      "source": [
        "from time import perf_counter\n",
        "\n",
        "model.reset_learning()\n",
        "trainer = TTTTrainer(model=model, config=LearningConfig())\n",
        "\n",
        "chunk_times = []\n",
        "last_time = perf_counter()\n",
        "\n",
        "def timing_callback(idx, total, loss):\n",
        "    global last_time\n",
        "    now = perf_counter()\n",
        "    elapsed = now - last_time\n",
        "    chunk_times.append(elapsed)\n",
        "    last_time = now\n",
        "    print(f\"  Chunk {idx+1}/{total}: {elapsed:.2f}s\")\n",
        "\n",
        "last_time = perf_counter()\n",
        "metrics = trainer.train_on_document(doc, progress_callback=timing_callback)\n",
        "\n",
        "avg_time = sum(chunk_times) / len(chunk_times) if chunk_times else 0\n",
        "print(f\"\\n✓ Latency test results:\")\n",
        "print(f\"  Average time per chunk: {avg_time:.2f}s\")\n",
        "print(f\"  Total learning time: {metrics.learning_time_seconds:.2f}s\")\n",
        "\n",
        "# Target: <3s per 2048-token chunk on T4\n",
        "assert avg_time < 3.0, f\"Average {avg_time:.2f}s exceeds 3s target!\"\n",
        "print(\"  ✓ Latency within target (<3s per chunk)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "latency-pass",
      "metadata": {
        "id": "latency-pass",
        "outputId": "714e2665-06e5-4bfa-960e-b9bf773559d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "✓ Step 8.4: Latency Test PASSED\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"✓ Step 8.4: Latency Test PASSED\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "all-pass",
      "metadata": {
        "id": "all-pass",
        "outputId": "c782dbc6-f2be-4af1-bc73-0788db69274d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "✓ ALL PHASE 8 INTEGRATION TESTS PASSED!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ ALL PHASE 8 INTEGRATION TESTS PASSED!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}