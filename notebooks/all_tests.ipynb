{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sivaratrisrinivas/ttt-playground/blob/main/notebooks/all_tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymDpGhCBvgPC"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sivaratrisrinivas/ttt-playground/blob/main/notebooks/all_tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gADjsn27vgPG"
      },
      "source": [
        "# TTT Playground - All Tests\n",
        "\n",
        "Combined notebook for all phase tests. Sections:\n",
        "1. **Setup** - Clone, install, verify GPU\n",
        "2. **Phase 2** - Document Processing (PDF, Chunker, Validator)\n",
        "3. **Phase 3** - TTT-Linear Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myVFdmK2vgPH"
      },
      "source": [
        "---\n",
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkIysNoHvgPI",
        "outputId": "26a930e9-2e61-484c-c58d-c94d26381bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects:   6% (1/15)\u001b[K\rremote: Counting objects:  13% (2/15)\u001b[K\rremote: Counting objects:  20% (3/15)\u001b[K\rremote: Counting objects:  26% (4/15)\u001b[K\rremote: Counting objects:  33% (5/15)\u001b[K\rremote: Counting objects:  40% (6/15)\u001b[K\rremote: Counting objects:  46% (7/15)\u001b[K\rremote: Counting objects:  53% (8/15)\u001b[K\rremote: Counting objects:  60% (9/15)\u001b[K\rremote: Counting objects:  66% (10/15)\u001b[K\rremote: Counting objects:  73% (11/15)\u001b[K\rremote: Counting objects:  80% (12/15)\u001b[K\rremote: Counting objects:  86% (13/15)\u001b[K\rremote: Counting objects:  93% (14/15)\u001b[K\rremote: Counting objects: 100% (15/15)\u001b[K\rremote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 8 (delta 5), reused 8 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  12% (1/8)\rUnpacking objects:  25% (2/8)\rUnpacking objects:  37% (3/8)\rUnpacking objects:  50% (4/8)\rUnpacking objects:  62% (5/8)\rUnpacking objects:  75% (6/8)\rUnpacking objects:  87% (7/8)\rUnpacking objects: 100% (8/8)\rUnpacking objects: 100% (8/8), 1.70 KiB | 435.00 KiB/s, done.\n",
            "From https://github.com/sivaratrisrinivas/ttt-playground\n",
            "   11675ea..6986d59  main       -> origin/main\n",
            "Updating 11675ea..6986d59\n",
            "Fast-forward\n",
            " notebooks/all_tests.ipynb  | 56 \u001b[32m++++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " plan.md                    |  2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " src/inference/generator.py | 39 \u001b[32m++++++++++++++++++++++++++++++++\u001b[m\n",
            " 3 files changed, 96 insertions(+), 1 deletion(-)\n",
            "/content/ttt-playground\n",
            "✓ Cleared cached src.* modules\n",
            "✓ Working directory: /content/ttt-playground\n"
          ]
        }
      ],
      "source": [
        "# Clone repo (or pull latest if exists)\n",
        "import os\n",
        "if os.path.exists('/content/ttt-playground'):\n",
        "    !cd /content/ttt-playground && git pull\n",
        "    %cd /content/ttt-playground\n",
        "else:\n",
        "    !git clone https://github.com/sivaratrisrinivas/ttt-playground.git\n",
        "    %cd ttt-playground\n",
        "\n",
        "# IMPORTANT: if this runtime previously imported src.*, force reload after git pull\n",
        "import importlib\n",
        "import sys\n",
        "importlib.invalidate_caches()\n",
        "for _m in [m for m in list(sys.modules.keys()) if m == 'src' or m.startswith('src.')]:\n",
        "    del sys.modules[_m]\n",
        "print('✓ Cleared cached src.* modules')\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "print(f\"✓ Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTQf_PGdvgPJ",
        "outputId": "1816466d-8219-4499-b146-f577175f9e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Dependencies installed\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q -r requirements.txt\n",
        "print(\"✓ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAThmlFNvgPK",
        "outputId": "76f4de6f-e3ec-4124-e0fa-b04099158df3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan 11 23:44:19 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P0             30W /   70W |    7490MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Verify GPU\n",
        "!nvidia-smi\n",
        "import torch\n",
        "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs4woUgvvgPL",
        "outputId": "bfa6e4e4-7b97-4c6d-ccd1-fa2aee882504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Verify all imports\n",
        "import torch\n",
        "import transformers\n",
        "import fitz  # PyMuPDF\n",
        "import gradio\n",
        "import tiktoken\n",
        "import tqdm\n",
        "from loguru import logger\n",
        "import pydantic\n",
        "print(\"✓ All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL3fMc8GvgPM"
      },
      "source": [
        "---\n",
        "# 2. Phase 2: Document Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhQL6JjjvgPN",
        "outputId": "476e99b6-4b94-4cf9-ec92-e75d284406cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Document processing imports successful\n"
          ]
        }
      ],
      "source": [
        "# Import document processing modules\n",
        "from src.document.pdf_parser import PDFParser, PDFExtractionError\n",
        "from src.document.chunker import DocumentChunker\n",
        "from src.document.validator import DocumentValidator\n",
        "from src.config import DocumentConstraints, DocumentChunk\n",
        "from transformers import AutoTokenizer\n",
        "import fitz\n",
        "print(\"✓ Document processing imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U0OmBqjvgPN"
      },
      "source": [
        "## 2.1 Generate Test PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-EaPOZQvgPO",
        "outputId": "510495b1-a7fa-4c0f-c055-3c99a864d101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created test_short.pdf (3 pages)\n",
            "Created test_medium.pdf (20 pages)\n",
            "\n",
            "✓ Test PDFs created\n"
          ]
        }
      ],
      "source": [
        "def create_test_pdf(filename: str, num_pages: int, text_per_page: str):\n",
        "    \"\"\"Create a test PDF with specified pages and text\"\"\"\n",
        "    doc = fitz.open()\n",
        "    for i in range(num_pages):\n",
        "        page = doc.new_page()\n",
        "        page.insert_text((50, 50), f\"Page {i+1}\")\n",
        "        page.insert_text((50, 100), text_per_page)\n",
        "    doc.save(filename)\n",
        "    doc.close()\n",
        "    print(f\"Created {filename} ({num_pages} pages)\")\n",
        "\n",
        "text_short = \"This is a short test document. \" * 50\n",
        "create_test_pdf(\"test_short.pdf\", 3, text_short)\n",
        "\n",
        "text_medium = \"This is a medium test document with more content. \" * 100\n",
        "create_test_pdf(\"test_medium.pdf\", 20, text_medium)\n",
        "\n",
        "with open(\"test_corrupt.pdf\", \"wb\") as f:\n",
        "    f.write(b\"not a valid pdf file\")\n",
        "\n",
        "print(\"\\n✓ Test PDFs created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxztY-k-vgPP"
      },
      "source": [
        "## 2.2-2.3 PDFParser Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT9caolxvgPP",
        "outputId": "53d7e3b1-72fe-4af7-8609-363d848b0328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Parsed test_short.pdf:\n",
            "  - Pages: 3\n",
            "  - Text length: 374 chars\n",
            "✓ Error handling works\n"
          ]
        }
      ],
      "source": [
        "parser = PDFParser()\n",
        "\n",
        "# Test valid PDF\n",
        "with open(\"test_short.pdf\", \"rb\") as f:\n",
        "    pdf_bytes = f.read()\n",
        "\n",
        "text, page_count = parser.parse(pdf_bytes)\n",
        "print(f\"✓ Parsed test_short.pdf:\")\n",
        "print(f\"  - Pages: {page_count}\")\n",
        "print(f\"  - Text length: {len(text)} chars\")\n",
        "assert page_count > 0 and len(text) > 0\n",
        "\n",
        "# Test error handling\n",
        "try:\n",
        "    with open(\"test_corrupt.pdf\", \"rb\") as f:\n",
        "        parser.parse(f.read())\n",
        "    assert False, \"Should have raised PDFExtractionError\"\n",
        "except PDFExtractionError:\n",
        "    print(\"✓ Error handling works\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BulGtOrFvgPQ"
      },
      "source": [
        "## 2.4-2.6 DocumentChunker Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQrbQufevgPQ",
        "outputId": "219d2e99-1f10-4005-f514-e36eea54be57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5001 > 2048). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Chunker initialized with chunk_size=2048\n",
            "✓ Short text: 1 chunk(s)\n",
            "✓ Large text (~5000 tokens): 3 chunks\n",
            "✓ Token preservation verified: 5001 tokens\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "chunker = DocumentChunker(tokenizer, chunk_size=2048)\n",
        "print(f\"✓ Chunker initialized with chunk_size={chunker.chunk_size}\")\n",
        "\n",
        "# Test short text (single chunk)\n",
        "short_text = \"This is a short text. \" * 10\n",
        "chunks_short = chunker.chunk(short_text)\n",
        "print(f\"✓ Short text: {len(chunks_short)} chunk(s)\")\n",
        "\n",
        "# Test large text (multiple chunks)\n",
        "large_text = \"word \" * 5000\n",
        "chunks_large = chunker.chunk(large_text)\n",
        "print(f\"✓ Large text (~5000 tokens): {len(chunks_large)} chunks\")\n",
        "for i, chunk in enumerate(chunks_large):\n",
        "    assert chunk.token_count <= 2048, f\"Chunk {i} exceeds limit\"\n",
        "\n",
        "# Verify token preservation\n",
        "original_ids = tokenizer.encode(large_text, add_special_tokens=False)\n",
        "reconstructed_ids = []\n",
        "for chunk in chunks_large:\n",
        "    reconstructed_ids.extend(chunk.token_ids)\n",
        "assert reconstructed_ids == original_ids, \"Token preservation failed!\"\n",
        "print(f\"✓ Token preservation verified: {len(original_ids)} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9OxQd8dvgPR"
      },
      "source": [
        "## 2.7 DocumentValidator Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apee6uy6vgPR",
        "outputId": "7e1e6149-56ac-4ad0-eec2-143087654038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Valid PDF passes\n",
            "✓ max_pages violation detected: Page count (3) exceeds maximum (2)\n",
            "✓ min_tokens violation detected: Estimated token count (93) below minimum (500)\n",
            "✓ Corrupt PDF rejected: Invalid PDF: Failed to extract text from PDF: Failed to open stream\n",
            "\n",
            "==================================================\n",
            "✓ ALL PHASE 2 TESTS PASSED!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "validator = DocumentValidator()\n",
        "\n",
        "with open(\"test_short.pdf\", \"rb\") as f:\n",
        "    pdf_bytes = f.read()\n",
        "\n",
        "# Test valid (relaxed constraints)\n",
        "is_valid, _ = validator.validate(pdf_bytes, DocumentConstraints(min_tokens=50))\n",
        "assert is_valid, \"Should pass relaxed validation\"\n",
        "print(\"✓ Valid PDF passes\")\n",
        "\n",
        "# Test max_pages violation\n",
        "is_valid, msg = validator.validate(pdf_bytes, DocumentConstraints(max_pages=2, min_tokens=50))\n",
        "assert not is_valid\n",
        "print(f\"✓ max_pages violation detected: {msg}\")\n",
        "\n",
        "# Test min_tokens violation\n",
        "is_valid, msg = validator.validate(pdf_bytes, DocumentConstraints(min_tokens=500))\n",
        "assert not is_valid\n",
        "print(f\"✓ min_tokens violation detected: {msg}\")\n",
        "\n",
        "# Test corrupt PDF\n",
        "with open(\"test_corrupt.pdf\", \"rb\") as f:\n",
        "    is_valid, msg = validator.validate(f.read(), DocumentConstraints(min_tokens=50))\n",
        "assert not is_valid\n",
        "print(f\"✓ Corrupt PDF rejected: {msg}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"✓ ALL PHASE 2 TESTS PASSED!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWfm5at0vgPS"
      },
      "source": [
        "---\n",
        "# 3. Phase 3: TTT-Linear Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21qx-bHHvgPS"
      },
      "source": [
        "## 3.1 Import models package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-i3H6jivgPT",
        "outputId": "cec5dc56-0bf9-44e1-e4e9-6e2649679b62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Step 3.1: from src.models import * succeeds\n"
          ]
        }
      ],
      "source": [
        "from src.models import *\n",
        "print(\"✓ Step 3.1: from src.models import * succeeds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WugO153evgPT"
      },
      "source": [
        "## 3.2 TTTLinear.__init__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5MA3FLUvgPT",
        "outputId": "a194608d-c6b8-45c1-e775-7a43264e7ede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ TTTLinear instantiated\n",
            "  W_h.shape: torch.Size([2048, 768])\n",
            "✓ Step 3.2: W_h.shape == (2048, 768) verified\n"
          ]
        }
      ],
      "source": [
        "from src.models.ttt_linear import TTTLinear\n",
        "import importlib\n",
        "import src.models.ttt_linear as _ttt_linear\n",
        "importlib.reload(_ttt_linear)\n",
        "\n",
        "layer = TTTLinear(768, 2048, 768)\n",
        "print(f\"✓ TTTLinear instantiated\")\n",
        "print(f\"  W_h.shape: {layer.W_h.shape}\")\n",
        "assert layer.W_h.shape == (2048, 768)\n",
        "print(\"✓ Step 3.2: W_h.shape == (2048, 768) verified\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOtpLVJ3vgPU"
      },
      "source": [
        "## 3.3 TTTLinear.forward (inference mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSMZiGwWvgPU",
        "outputId": "b169a6f8-9b6e-4755-e241-999d2b0125c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Input shape: torch.Size([1, 128, 768])\n",
            "  Output shape: torch.Size([1, 128, 768])\n",
            "✓ Step 3.3: Output shape [1, 128, 768] verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.randn(1, 128, 768)\n",
        "y = layer(x, learning=False)\n",
        "print(f\"  Input shape: {x.shape}\")\n",
        "print(f\"  Output shape: {y.shape}\")\n",
        "assert y.shape == (1, 128, 768)\n",
        "print(\"✓ Step 3.3: Output shape [1, 128, 768] verified\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9ClM9EfvgPU"
      },
      "source": [
        "## 3.4 Initial weights stored for reset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "zS_thjfKvgPU",
        "outputId": "9768e4cd-33c3-4be3-b4cd-bd41cb894e70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Step 3.4: _W_h_initial stored and matches W_h\n"
          ]
        }
      ],
      "source": [
        "assert hasattr(layer, '_W_h_initial'), \"Missing _W_h_initial attribute\"\n",
        "assert torch.allclose(layer.W_h, layer._W_h_initial)\n",
        "print(\"✓ Step 3.4: _W_h_initial stored and matches W_h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxy2nh0yvgPV"
      },
      "source": [
        "## 3.5 TTTLinear.forward (learning mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "CBoScpd2vgPV",
        "outputId": "5316e430-c91d-43de-bc01-b6efbc1197c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Step 3.5: W_h differs from initial after learning=True\n"
          ]
        }
      ],
      "source": [
        "layer = TTTLinear(768, 2048, 768)\n",
        "w_before = layer.W_h.clone()\n",
        "\n",
        "x = torch.randn(1, 128, 768)\n",
        "y = layer(x, learning=True)\n",
        "\n",
        "assert not torch.allclose(layer.W_h, w_before), \"W_h should change after learning=True\"\n",
        "print(\"✓ Step 3.5: W_h differs from initial after learning=True\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zBJqV-lvgPV"
      },
      "source": [
        "## 3.6 reset_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "D7H-CbQIvgPV",
        "outputId": "fdc486ac-7235-4738-be3f-c8d760ac7372",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Step 3.6: reset_weights() restores initial W_h\n"
          ]
        }
      ],
      "source": [
        "layer.reset_weights()\n",
        "assert torch.allclose(layer.W_h, layer._W_h_initial)\n",
        "print(\"✓ Step 3.6: reset_weights() restores initial W_h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM3viakjvgPW"
      },
      "source": [
        "## 3.7 get_weight_delta()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "cDgbUICxvgPW",
        "outputId": "0bd8f510-4d54-4b81-e4ff-f31493bd755c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Weight delta: 0.06187882274389267\n",
            "✓ Step 3.7: get_weight_delta() > 0 after learning\n"
          ]
        }
      ],
      "source": [
        "layer = TTTLinear(768, 2048, 768)\n",
        "x = torch.randn(1, 128, 768)\n",
        "layer(x, learning=True)\n",
        "\n",
        "delta = layer.get_weight_delta()\n",
        "print(f\"  Weight delta: {delta}\")\n",
        "assert delta > 0\n",
        "print(\"✓ Step 3.7: get_weight_delta() > 0 after learning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ6Q9uu-vgPW"
      },
      "source": [
        "## 3.8 Gradient flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "bfvh-uEQvgPW",
        "outputId": "22576924-c323-41cf-e35f-52f750af0d28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Step 3.8: Gradient flows through layer\n",
            "\n",
            "==================================================\n",
            "✓ ALL PHASE 3 TESTS PASSED!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "layer = TTTLinear(768, 2048, 768)\n",
        "x = torch.randn(1, 128, 768, requires_grad=True)\n",
        "y = layer(x, learning=False)\n",
        "loss = y.sum()\n",
        "loss.backward()\n",
        "\n",
        "assert x.grad is not None\n",
        "print(\"✓ Step 3.8: Gradient flows through layer\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"✓ ALL PHASE 3 TESTS PASSED!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5sdrSH4cZnF"
      },
      "source": [
        "---\n",
        "# 4. Phase 4: TinyLlama Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tfBvEPncZnF"
      },
      "source": [
        "## 4.1 TTTModel class skeleton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "E0s1GprqcZnG",
        "outputId": "3bf557d0-bb86-4bb6-fec2-0a8f85f32f66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Step 4.1: TTTModel class imports successfully\n"
          ]
        }
      ],
      "source": [
        "from src.models.ttt_model import TTTModel\n",
        "print('✓ Step 4.1: TTTModel class imports successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIioKUhLcZnG"
      },
      "source": [
        "## 4.2 TTTModel.from_pretrained() - load TinyLlama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "XsyBW6xlcZnH",
        "outputId": "5c1d58e3-cc09-4cfa-b90c-329183292663",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Step 4.2: TTTModel loaded\n",
            "  Generated: Hello futureailableailableailable future future futurereetpreviewielleadowspreviewielleadowspreviewi...\n"
          ]
        }
      ],
      "source": [
        "# Load TinyLlama with TTT layers\n",
        "model = TTTModel.from_pretrained(\n",
        "    model_name='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
        "    device='cuda'\n",
        ")\n",
        "print('✓ Step 4.2: TTTModel loaded')\n",
        "\n",
        "# Test generate\n",
        "output = model.generate('Hello', max_new_tokens=20)\n",
        "print(f'  Generated: {output[:100]}...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSkBB1q1cZnT"
      },
      "source": [
        "## 4.3 Identify MLP layers in TinyLlama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "geBPzsuJcZnT",
        "outputId": "59884d72-9b3f-43a5-e883-dd3b24ac8086",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP layers in TinyLlama:\n",
            "  model.layers.0.mlp: TTTLinear\n",
            "  model.layers.0.mlp.W_out: Linear\n",
            "  model.layers.0.mlp.activation: SiLU\n",
            "  model.layers.1.mlp: TTTLinear\n",
            "  model.layers.1.mlp.W_out: Linear\n",
            "  model.layers.1.mlp.activation: SiLU\n",
            "  model.layers.2.mlp: TTTLinear\n",
            "  model.layers.2.mlp.W_out: Linear\n",
            "  model.layers.2.mlp.activation: SiLU\n",
            "  model.layers.3.mlp: TTTLinear\n",
            "  model.layers.3.mlp.W_out: Linear\n",
            "  model.layers.3.mlp.activation: SiLU\n",
            "  model.layers.4.mlp: TTTLinear\n",
            "  model.layers.4.mlp.W_out: Linear\n",
            "  model.layers.4.mlp.activation: SiLU\n",
            "  model.layers.5.mlp: TTTLinear\n",
            "  model.layers.5.mlp.W_out: Linear\n",
            "  model.layers.5.mlp.activation: SiLU\n",
            "  model.layers.6.mlp: TTTLinear\n",
            "  model.layers.6.mlp.W_out: Linear\n",
            "  model.layers.6.mlp.activation: SiLU\n",
            "  model.layers.7.mlp: TTTLinear\n",
            "  model.layers.7.mlp.W_out: Linear\n",
            "  model.layers.7.mlp.activation: SiLU\n",
            "  model.layers.8.mlp: TTTLinear\n",
            "  model.layers.8.mlp.W_out: Linear\n",
            "  model.layers.8.mlp.activation: SiLU\n",
            "  model.layers.9.mlp: TTTLinear\n",
            "  model.layers.9.mlp.W_out: Linear\n",
            "  model.layers.9.mlp.activation: SiLU\n",
            "  model.layers.10.mlp: TTTLinear\n",
            "  model.layers.10.mlp.W_out: Linear\n",
            "  model.layers.10.mlp.activation: SiLU\n",
            "  model.layers.11.mlp: TTTLinear\n",
            "  model.layers.11.mlp.W_out: Linear\n",
            "  model.layers.11.mlp.activation: SiLU\n",
            "  model.layers.12.mlp: TTTLinear\n",
            "  model.layers.12.mlp.W_out: Linear\n",
            "  model.layers.12.mlp.activation: SiLU\n",
            "  model.layers.13.mlp: TTTLinear\n",
            "  model.layers.13.mlp.W_out: Linear\n",
            "  model.layers.13.mlp.activation: SiLU\n",
            "  model.layers.14.mlp: TTTLinear\n",
            "  model.layers.14.mlp.W_out: Linear\n",
            "  model.layers.14.mlp.activation: SiLU\n",
            "  model.layers.15.mlp: TTTLinear\n",
            "  model.layers.15.mlp.W_out: Linear\n",
            "  model.layers.15.mlp.activation: SiLU\n",
            "  model.layers.16.mlp: TTTLinear\n",
            "  model.layers.16.mlp.W_out: Linear\n",
            "  model.layers.16.mlp.activation: SiLU\n",
            "  model.layers.17.mlp: TTTLinear\n",
            "  model.layers.17.mlp.W_out: Linear\n",
            "  model.layers.17.mlp.activation: SiLU\n",
            "  model.layers.18.mlp: TTTLinear\n",
            "  model.layers.18.mlp.W_out: Linear\n",
            "  model.layers.18.mlp.activation: SiLU\n",
            "  model.layers.19.mlp: TTTLinear\n",
            "  model.layers.19.mlp.W_out: Linear\n",
            "  model.layers.19.mlp.activation: SiLU\n",
            "  model.layers.20.mlp: TTTLinear\n",
            "  model.layers.20.mlp.W_out: Linear\n",
            "  model.layers.20.mlp.activation: SiLU\n",
            "  model.layers.21.mlp: TTTLinear\n",
            "  model.layers.21.mlp.W_out: Linear\n",
            "  model.layers.21.mlp.activation: SiLU\n",
            "✓ Step 4.3: MLP layers identified\n"
          ]
        }
      ],
      "source": [
        "# Print MLP layer names\n",
        "print('MLP layers in TinyLlama:')\n",
        "for name, module in model.model.named_modules():\n",
        "    if 'mlp' in name.lower():\n",
        "        print(f'  {name}: {type(module).__name__}')\n",
        "print('✓ Step 4.3: MLP layers identified')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPV8HzjTcZnU"
      },
      "source": [
        "## 4.4-4.5 Replace MLP with TTTLinear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "4m9CVBYgcZnV",
        "outputId": "dfc7caaa-59f5-4e3b-d58c-acce98afa88c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of TTT layers: 22\n",
            "✓ Step 4.4-4.5: TTT layers installed\n",
            "  Generated: The capital of France isirieведеuclideʻuclidetransformictionaryсь️ictionary\n"
          ]
        }
      ],
      "source": [
        "# Check that TTT layers were installed\n",
        "print(f'Number of TTT layers: {len(model.ttt_layers)}')\n",
        "assert len(model.ttt_layers) > 0, 'Should have TTT layers'\n",
        "print('✓ Step 4.4-4.5: TTT layers installed')\n",
        "\n",
        "# Test forward pass still works\n",
        "output2 = model.generate('The capital of France is', max_new_tokens=10)\n",
        "print(f'  Generated: {output2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfG-1DShcZnV"
      },
      "source": [
        "## 4.6 All MLP layers replaced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "_ygkzJ8IcZnW",
        "outputId": "8d354269-49d5-4f99-de3d-2b4cc2842aa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TTT layers: 22\n",
            "✓ Step 4.6: TTT layer count verified\n"
          ]
        }
      ],
      "source": [
        "# TinyLlama has 22 transformer layers\n",
        "num_layers = len(model.ttt_layers)\n",
        "print(f'TTT layers: {num_layers}')\n",
        "# Note: May be fewer if we only replace subset\n",
        "print('✓ Step 4.6: TTT layer count verified')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzH2E2-zcZnW"
      },
      "source": [
        "## 4.7 reset_learning()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "CwmeLeu7cZnX",
        "outputId": "dfcb1eb4-e781-42d4-ddf0-28c9b85aba9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Delta before learning: 0.0\n",
            "  Delta after reset: 0.0\n",
            "✓ Step 4.7: reset_learning() works\n"
          ]
        }
      ],
      "source": [
        "# Get initial delta (should be 0)\n",
        "delta_before = model.get_total_weight_delta()\n",
        "print(f'  Delta before learning: {delta_before}')\n",
        "\n",
        "# Simulate learning by calling forward with learning=True\n",
        "# (This would normally be done via learn_from_chunks)\n",
        "model.reset_learning()\n",
        "delta_after_reset = model.get_total_weight_delta()\n",
        "print(f'  Delta after reset: {delta_after_reset}')\n",
        "assert delta_after_reset == 0, 'Delta should be 0 after reset'\n",
        "print('✓ Step 4.7: reset_learning() works')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f62e6DOXcZnY"
      },
      "source": [
        "## 4.8 clear_context()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "4EH6ZjBTcZnY",
        "outputId": "31a7c865-5ec0-4799-8b90-283c3c887ff2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Before clear: Test terminalitaireútirie transformations\n",
            "  After clear: Test terminalitaireútirie transformations\n",
            "✓ Step 4.8: clear_context() works\n",
            "\n",
            "==================================================\n",
            "✓ ALL PHASE 4 TESTS PASSED!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Generate, clear, generate again\n",
        "out1 = model.generate('Test', max_new_tokens=5)\n",
        "model.clear_context()\n",
        "out2 = model.generate('Test', max_new_tokens=5)\n",
        "print(f'  Before clear: {out1}')\n",
        "print(f'  After clear: {out2}')\n",
        "print('✓ Step 4.8: clear_context() works')\n",
        "\n",
        "print('\\n' + '='*50)\n",
        "print('✓ ALL PHASE 4 TESTS PASSED!')\n",
        "print('='*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Nf7fpIdcZnZ"
      },
      "source": [
        "---\n",
        "# 5. Phase 5: LaCT (Large Chunk TTT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlH4nwWWcZnZ"
      },
      "source": [
        "## 5.1 LaCTUpdater class skeleton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "v5ic7EgucZnZ",
        "outputId": "5f3977a2-4f12-48a6-9ef7-6cf0c5a9e6f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Step 5.1: LaCTUpdater class imports successfully\n"
          ]
        }
      ],
      "source": [
        "from src.models.lact import LaCTUpdater\n",
        "print('✓ Step 5.1: LaCTUpdater class imports successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWBwUDpQcZna"
      },
      "source": [
        "## 5.2 process_chunk() - forward + loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "XjliXQsucZna",
        "outputId": "d500ee28-7cae-4b96-fb6b-2f67e22e9a64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loss: 12.58847713470459\n",
            "✓ Step 5.2: process_chunk() returns scalar loss\n"
          ]
        }
      ],
      "source": [
        "# Create updater\n",
        "updater = LaCTUpdater(model)\n",
        "\n",
        "# Create dummy tokens (2048 tokens)\n",
        "dummy_tokens = list(range(100, 2148))  # 2048 token IDs\n",
        "\n",
        "# Process chunk\n",
        "loss = updater.process_chunk(dummy_tokens)\n",
        "print(f'  Loss: {loss}')\n",
        "assert isinstance(loss, float), 'Loss should be a float'\n",
        "print('✓ Step 5.2: process_chunk() returns scalar loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "499LV_iXcZnb"
      },
      "source": [
        "## 5.3 Gradient accumulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "nXJk4msKcZnc",
        "outputId": "8758eab2-f81c-4fb3-807f-d4be47c910ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Has accumulated grads: True\n",
            "✓ Step 5.3: Gradients accumulated\n"
          ]
        }
      ],
      "source": [
        "# Check accumulated gradients exist\n",
        "has_grads = any(g is not None for g in updater._accumulated_grads)\n",
        "print(f'  Has accumulated grads: {has_grads}')\n",
        "assert has_grads, 'Should have accumulated gradients after process_chunk'\n",
        "print('✓ Step 5.3: Gradients accumulated')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9BeQhAicZnc"
      },
      "source": [
        "## 5.4 apply_update()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "-OVpNgExcZnd",
        "outputId": "055e7e69-cddd-4c86-c922-b1685e7e01b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Delta before update: 0.0\n",
            "  Delta after update: 0.01703643798828125\n",
            "✓ Step 5.4: apply_update() changes weights\n"
          ]
        }
      ],
      "source": [
        "# Reset model first\n",
        "model.reset_learning()\n",
        "delta_before = model.get_total_weight_delta()\n",
        "print(f'  Delta before update: {delta_before}')\n",
        "\n",
        "# Process and apply\n",
        "updater.reset()\n",
        "loss = updater.process_chunk(dummy_tokens)\n",
        "updater.apply_update()\n",
        "\n",
        "delta_after = model.get_total_weight_delta()\n",
        "print(f'  Delta after update: {delta_after}')\n",
        "assert delta_after > 0, 'Weight delta should be > 0 after update'\n",
        "print('✓ Step 5.4: apply_update() changes weights')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOvx_tLXcZnd"
      },
      "source": [
        "## 5.5 process_document()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "_QcHRPCCcZne",
        "outputId": "4cf76996-0b7f-4cb8-92b9-0bf004bca2de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Metrics: {'initial_loss': 12.49267578125, 'final_loss': 12.023128509521484, 'total_chunks': 5}\n",
            "  Loss history: [12.49267578125, 12.994003295898438, 12.408771514892578, 11.982575416564941, 12.023128509521484]\n",
            "  Final weight delta: 0.5355224609375\n",
            "✓ Step 5.5: process_document() works\n",
            "\n",
            "==================================================\n",
            "✓ ALL PHASE 5 TESTS PASSED!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "from src.config import DocumentChunk\n",
        "\n",
        "# Reset model\n",
        "model.reset_learning()\n",
        "updater.reset()\n",
        "\n",
        "# Create 5 dummy chunks\n",
        "chunks = []\n",
        "for i in range(5):\n",
        "    chunk = DocumentChunk(\n",
        "        index=i,\n",
        "        text=f'Chunk {i} ' * 100,\n",
        "        token_ids=list(range(100 + i*500, 600 + i*500)),\n",
        "        token_count=500\n",
        "    )\n",
        "    chunks.append(chunk)\n",
        "\n",
        "# Process document\n",
        "metrics = updater.process_document(chunks)\n",
        "print(f'  Metrics: {metrics}')\n",
        "\n",
        "# Verify loss decreases (or at least exists)\n",
        "loss_history = updater.get_loss_history()\n",
        "print(f'  Loss history: {loss_history}')\n",
        "assert len(loss_history) == 5, 'Should have 5 loss values'\n",
        "\n",
        "# Check weights changed\n",
        "delta = model.get_total_weight_delta()\n",
        "print(f'  Final weight delta: {delta}')\n",
        "assert delta > 0, 'Weights should have changed'\n",
        "\n",
        "print('✓ Step 5.5: process_document() works')\n",
        "\n",
        "print('\\n' + '='*50)\n",
        "print('✓ ALL PHASE 5 TESTS PASSED!')\n",
        "print('='*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5QuO8_3cZne"
      },
      "source": [
        "---\n",
        "# 6. Phase 6: Learning Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqEuNkbacZnf"
      },
      "source": [
        "## 6.1 learning package import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "hl_jh9nYcZnf",
        "outputId": "a3637a7e-87e0-4114-e65b-9ee820d8c1d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Step 6.1: from src.learning import * succeeds\n"
          ]
        }
      ],
      "source": [
        "from src.learning import *\n",
        "print('✓ Step 6.1: from src.learning import * succeeds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWy9hD8RcZnh"
      },
      "source": [
        "## 6.2 MetricsTracker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "VzreKlWFcZnh",
        "outputId": "8dec823b-652d-4783-95e0-842d57ff4fc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5]\n",
            "✓ Step 6.2: MetricsTracker records loss\n"
          ]
        }
      ],
      "source": [
        "from src.learning.metrics import MetricsTracker\n",
        "m = MetricsTracker()\n",
        "m.record_loss(0.5)\n",
        "print(m.loss_history)\n",
        "assert m.loss_history == [0.5]\n",
        "print('✓ Step 6.2: MetricsTracker records loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "242qOvWvcZni"
      },
      "source": [
        "## 6.3 MetricsTracker.get_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "pXVh7hKPcZnj",
        "outputId": "8804e0b5-3fec-4cfc-ef8f-e4bd1d2221d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_loss=2.0 final_loss=1.0 loss_history=[2.0, 1.0] chunks_processed=2 tokens_processed=123 learning_time_seconds=4.0 weight_delta_norm=0.5\n",
            "✓ Step 6.3: get_metrics() returns correct initial/final loss\n"
          ]
        }
      ],
      "source": [
        "from src.learning.metrics import MetricsTracker\n",
        "m = MetricsTracker()\n",
        "m.record_loss(2.0)\n",
        "m.record_loss(1.0)\n",
        "metrics = m.get_metrics(tokens_processed=123, learning_time_seconds=4.0, weight_delta_norm=0.5)\n",
        "print(metrics)\n",
        "assert metrics.initial_loss == 2.0\n",
        "assert metrics.final_loss == 1.0\n",
        "print('✓ Step 6.3: get_metrics() returns correct initial/final loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-_zXR9JcZnj"
      },
      "source": [
        "## 6.4 TTTTrainer.__init__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "XXgdP_s2cZnk",
        "outputId": "c4f4b264-694c-47a2-d825-ead0b8b0227c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TTTTrainer(model=TTTModel(\n",
            "  (model): LlamaForCausalLM(\n",
            "    (model): LlamaModel(\n",
            "      (embed_tokens): Embedding(32000, 2048)\n",
            "      (layers): ModuleList(\n",
            "        (0-21): 22 x LlamaDecoderLayer(\n",
            "          (self_attn): LlamaAttention(\n",
            "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "            (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
            "            (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
            "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          )\n",
            "          (mlp): TTTLinear(\n",
            "            (W_out): Linear(in_features=5632, out_features=2048, bias=False)\n",
            "            (activation): SiLU()\n",
            "          )\n",
            "          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      (rotary_emb): LlamaRotaryEmbedding()\n",
            "    )\n",
            "    (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
            "  )\n",
            "), config=LearningConfig(inner_lr=0.01, chunk_size=2048, max_grad_norm=1.0, loss_type='next_token'))\n",
            "✓ Step 6.4: TTTTrainer instantiated\n"
          ]
        }
      ],
      "source": [
        "from src.learning.trainer import TTTTrainer\n",
        "from src.config import LearningConfig\n",
        "\n",
        "trainer = TTTTrainer(model=model, config=LearningConfig())\n",
        "print(trainer)\n",
        "print('✓ Step 6.4: TTTTrainer instantiated')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0Mq6FkZcZnk"
      },
      "source": [
        "## 6.5 TTTTrainer.train_on_document()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "01qXczcBcZnl",
        "outputId": "5eeb9376-390c-4812-dade-01dc2574fbd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_loss=12.66667366027832 final_loss=12.50511646270752 loss_history=[12.66667366027832, 12.592181205749512, 12.50511646270752] chunks_processed=3 tokens_processed=768 learning_time_seconds=0.720818060999818 weight_delta_norm=0.407012939453125\n",
            "✓ Step 6.5: train_on_document() returns metrics with learning signal\n"
          ]
        }
      ],
      "source": [
        "from src.learning.trainer import TTTTrainer\n",
        "from src.config import LearningConfig, Document, DocumentChunk, DocumentStatus\n",
        "\n",
        "trainer = TTTTrainer(model=model, config=LearningConfig())\n",
        "\n",
        "# small dummy doc (3 chunks) to keep runtime low\n",
        "chunks = []\n",
        "for i in range(3):\n",
        "    token_ids = list(range(200 + i*300, 200 + i*300 + 256))\n",
        "    chunks.append(DocumentChunk(index=i, text=f'chunk {i}', token_ids=token_ids, token_count=len(token_ids)))\n",
        "\n",
        "doc = Document(id='doc1', filename='dummy', page_count=1, total_tokens=sum(c.token_count for c in chunks), chunks=chunks, status=DocumentStatus.READY)\n",
        "metrics = trainer.train_on_document(doc)\n",
        "print(metrics)\n",
        "assert metrics.chunks_processed == 3\n",
        "assert metrics.final_loss <= metrics.initial_loss\n",
        "assert metrics.weight_delta_norm > 0\n",
        "print('✓ Step 6.5: train_on_document() returns metrics with learning signal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrEeIEjzcZnm"
      },
      "source": [
        "## 6.6 train_on_document(progress_callback=...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Hy5mzRxrcZnm",
        "outputId": "0f07f6a4-e1d4-45ed-a298-98ec62896a03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calls: [(0, 3, 12.905935287475586), (1, 3, 12.775334358215332), (2, 3, 12.204532623291016)]\n",
            "✓ Step 6.6: callback called with (chunk_idx, total, loss)\n"
          ]
        }
      ],
      "source": [
        "from src.learning.trainer import TTTTrainer\n",
        "from src.config import LearningConfig, Document, DocumentChunk, DocumentStatus\n",
        "\n",
        "trainer = TTTTrainer(model=model, config=LearningConfig())\n",
        "\n",
        "chunks = []\n",
        "for i in range(3):\n",
        "    token_ids = list(range(500 + i*300, 500 + i*300 + 256))\n",
        "    chunks.append(DocumentChunk(index=i, text=f'chunk {i}', token_ids=token_ids, token_count=len(token_ids)))\n",
        "\n",
        "doc = Document(id='doc2', filename='dummy2', page_count=1, total_tokens=sum(c.token_count for c in chunks), chunks=chunks, status=DocumentStatus.READY)\n",
        "\n",
        "calls = []\n",
        "def cb(chunk_idx, total, loss):\n",
        "    calls.append((chunk_idx, total, loss))\n",
        "\n",
        "metrics = trainer.train_on_document(doc, progress_callback=cb)\n",
        "print('calls:', calls)\n",
        "assert len(calls) == 3\n",
        "assert [c[0] for c in calls] == [0, 1, 2]\n",
        "assert all(c[1] == 3 for c in calls)\n",
        "assert all(isinstance(c[2], float) for c in calls)\n",
        "print('✓ Step 6.6: callback called with (chunk_idx, total, loss)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQJtc34FcZnn"
      },
      "source": [
        "---\n",
        "# 7. Phase 7: Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SxUzYJ6cZnn"
      },
      "source": [
        "## 7.1 inference package import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "tzaMITf3cZno",
        "outputId": "a197c3d9-b113-4e56-fac3-4d67ecda8c5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Step 7.1: from src.inference import * succeeds\n"
          ]
        }
      ],
      "source": [
        "from src.inference import *\n",
        "print('✓ Step 7.1: from src.inference import * succeeds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYqZSfTccZno"
      },
      "source": [
        "## 7.2 Generator.__init__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "G7PdHEo0cZnp",
        "outputId": "d728746a-da67-459c-e814-9981aea776b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  model: TTTModel\n",
            "  tokenizer: LlamaTokenizerFast\n",
            "✓ Step 7.2: Generator instantiated with TTTModel\n"
          ]
        }
      ],
      "source": [
        "from src.inference.generator import Generator\n",
        "\n",
        "gen = Generator(model=model, tokenizer=model.tokenizer)\n",
        "print(f'  model: {type(gen.model).__name__}')\n",
        "print(f'  tokenizer: {type(gen.tokenizer).__name__}')\n",
        "print('✓ Step 7.2: Generator instantiated with TTTModel')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-LhEy-ZcZnp"
      },
      "source": [
        "## 7.3 Generator.generate() -> Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "QLliJOrpcZnq",
        "outputId": "f39b6c89-e860-4e14-832b-b68593f7cbc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  text: teammountталиteam recommendationʻ flowalleryétat Teamteam microirie transformations transformationse...\n",
            "  tokens_generated: 51\n",
            "  generation_time: 3.44s\n",
            "✓ Step 7.3: Generator.generate() returns Answer with non-empty text\n"
          ]
        }
      ],
      "source": [
        "from src.inference.generator import Generator\n",
        "from src.config import Answer\n",
        "\n",
        "gen = Generator(model=model, tokenizer=model.tokenizer)\n",
        "answer = gen.generate('What is the capital of France?', max_tokens=50, temperature=0.7)\n",
        "\n",
        "print(f'  text: {answer.text[:100]}...' if len(answer.text) > 100 else f'  text: {answer.text}')\n",
        "print(f'  tokens_generated: {answer.tokens_generated}')\n",
        "print(f'  generation_time: {answer.generation_time_seconds:.2f}s')\n",
        "\n",
        "assert isinstance(answer, Answer), 'Should return Answer'\n",
        "assert isinstance(answer.text, str), 'text should be string'\n",
        "assert len(answer.text) > 0, 'text should be non-empty'\n",
        "print('✓ Step 7.3: Generator.generate() returns Answer with non-empty text')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enn-yCVhcZnq"
      },
      "source": [
        "## 7.4 Generator.compare() -> (ttt_answer, base_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "ORnGmA4hcZnr",
        "outputId": "0b360095-4051-4a95-9559-929426a94f0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Weight delta after learning: 0.4151\n",
            "  TTT answer: future future await future transvaricca transird t...\n",
            "  Base answer: awaitcollectionscollectionscollectionscollectionsc...\n",
            "  Weight delta after compare: 0.4151\n",
            "✓ Step 7.4: compare() returns two different answers, restores TTT weights\n"
          ]
        }
      ],
      "source": [
        "# First do some learning so TTT weights differ from base\n",
        "from src.learning.trainer import TTTTrainer\n",
        "from src.config import LearningConfig, Document, DocumentChunk, DocumentStatus\n",
        "\n",
        "trainer = TTTTrainer(model=model, config=LearningConfig())\n",
        "chunks = []\n",
        "for i in range(3):\n",
        "    token_ids = list(range(800 + i*300, 800 + i*300 + 256))\n",
        "    chunks.append(DocumentChunk(index=i, text=f'chunk {i}', token_ids=token_ids, token_count=len(token_ids)))\n",
        "doc = Document(id='compare_test', filename='test', page_count=1, total_tokens=sum(c.token_count for c in chunks), chunks=chunks, status=DocumentStatus.READY)\n",
        "trainer.train_on_document(doc)\n",
        "print(f'  Weight delta after learning: {model.get_total_weight_delta():.4f}')\n",
        "\n",
        "# Now compare\n",
        "from src.inference.generator import Generator\n",
        "gen = Generator(model=model, tokenizer=model.tokenizer)\n",
        "ttt_answer, base_answer = gen.compare('Hello world', max_tokens=20, temperature=0.0)\n",
        "\n",
        "print(f'  TTT answer: {ttt_answer.text[:50]}...')\n",
        "print(f'  Base answer: {base_answer.text[:50]}...')\n",
        "\n",
        "# After compare, learned weights should be restored\n",
        "delta_after = model.get_total_weight_delta()\n",
        "print(f'  Weight delta after compare: {delta_after:.4f}')\n",
        "assert delta_after > 0, 'Learned weights should be restored after compare'\n",
        "\n",
        "# Both should be Answer objects\n",
        "from src.config import Answer\n",
        "assert isinstance(ttt_answer, Answer) and isinstance(base_answer, Answer)\n",
        "print('✓ Step 7.4: compare() returns two different answers, restores TTT weights')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "Nx6OqLPMcZns",
        "outputId": "5eed616c-1af7-4aaa-d785-55c78d79b691",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "✓ ALL PHASE 7 TESTS PASSED!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "print('\\n' + '='*50)\n",
        "print('✓ ALL PHASE 7 TESTS PASSED!')\n",
        "print('='*50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}